<!DOCTYPE HTML>
<html lang="en">
  <head>
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-4F8XTXPF4E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-4F8XTXPF4E');
  </script>


    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Mengdi Li</title>

    <meta name="author" content="Mengdi Li">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="images/shaka-sign.svg" sizes="any" type="image/svg+xml">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">Mengdi Li / 李梦迪</p>
                <p>I am a research associate at the <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm.html">Knowledge Technology</a> group at the <a href="https://www.uni-hamburg.de/">University of Hamburg</a> (Germany), where I work on reinforcement learning and robotics.
                  Before that, I obtained my Master's degree in Computer Science and Bachelor's degree in Electronic Engineer at the <a href="https://en.cau.edu.cn/">China Agricutural University</a>. 
                  Please feel free to leave anonymous feedback and suggestions about both my work and me <a href="https://forms.gle/fVqvxmVVZdUN37QGA">here</a>. 
                </p>
                <p></p>
                <p style="text-align:center">
                  <a href="mailto:li_mengdi@hotmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/mengdi_li_cv.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=0W7UjrcAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/limengdi2">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mengdi-li/">Github</a> &nbsp;/&nbsp;
                  <a href="https://linkedin.com/in/limengdi2">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/mengdi_photo_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/mengdi_photo_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                  <li>
                    I am looking for job positions in RLHF for LLMs alighnment, and controlable LLMs. 
                    I would love to chat with you more if you think I would be a good candidate.
                  </li>
                  <li>
                    Our paper on LLMs for robotic multimodal exploration got accepted to IROS 2023. 
                  </li>
                  <li>
                    Our paper on stabalizing RL when the reward is from a joint training reward model got accepted to ICML 2023.
                  </li>
                </ul>
                
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm working on reinforcement learning with reward models, LLMs, and embodied agents.  
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Selected Publications</h2> 
                <a href="https://scholar.google.com/citations?user=0W7UjrcAAAAJ">(see all publications here)</a>
                <!-- <p>( <a href="https://scholar.google.com/citations?user=0W7UjrcAAAAJ">Full publication list</a> ) </p> -->
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/matcha-thumbnail.jpg' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Chat with the Environment: Interactive Multimodal Perception using Large Language Models</span>
          <br>
          <a href="https://xf-zhao.github.io/">Xufeng Zhao</a>,
          <strong>Mengdi Li</strong>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html">Cornelius Weber</a>,
          <a href="https://www.mbhafez.com/">Burhan Hafez</a>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan Wermter</a>,
          <br>
          <em>IROS</em>, 2023
          <br>
          <a href="https://matcha-model.github.io/">project page</a>
          /
          <a href="https://github.com/xf-zhao/Matcha">code</a>
          /
          <a href="https://youtu.be/rMMeMTWmT0k">video</a>
          /
          <a href="https://arxiv.org/abs/2303.08268">arXiv</a>
          <p></p>
          <p>
          We develop an LLM-centered modular network to provide high-level planning and reasoning skills and control interactive robot behaviour in a multimodal environment. 
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/irrl-unstable-training-loop.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Internally Rewarded Reinforcement Learning </span>
          <br>
          <strong>Mengdi Li*</strong>,
          <a href="https://xf-zhao.github.io/">Xufeng Zhao*</a>,
          <a href="https://jaeheelee.gitlab.io/">Jae Hee Lee</a>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html">Cornelius Weber</a>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan Wermter</a>,
          <br>
          <em>ICML</em>, 2023
          <br>
          <a href="https://ir-rl.github.io/">project page</a>
          /
          <a href="https://github.com/mengdi-li/internally-rewarded-rl">code</a>
          /
          <a href="https://arxiv.org/abs/2302.00270">arXiv</a>
          <p></p>
          <p>
          We propose the clipped linear reward to stablize reinforcement learning where reward signals for policy learning are generated by a discriminator-based reward model that is dependent on and jointly optimized with the policy.
          </p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/occlusion-reasoning-thumbnail.jpg' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Robotic Occlusion Reasoning for Efficient Object Existence Prediction</span>
          <br>
          <strong>Mengdi Li</strong>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html">Cornelius Weber</a>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/kerzel.html">Matthias Kerzel</a>,
          <a href="https://jaeheelee.gitlab.io/">Jae Hee Lee</a>,
          <a href="https://ellenzzn.github.io/zhenizeng/">Zheni Zeng</a>,
          <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan Wermter</a>,
          <br>
          <em>IROS</em>, 2021
          <br>
          <a href="https://github.com/mengdi-li/robotic-occlusion-reasoning">code</a>
          /
          <a href="https://arxiv.org/abs/2107.12095">arXiv</a>
          <p></p>
          <p>
          We propose an RNN-based model that is jointly trained with supervised and reinforcement learning to achieve the task of predicting the existence of objects in occusion scenarios.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/irrelevance-vqa-thumbnail.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Neural Networks for Detecting Irrelevant Questions During Visual Question Answering</span>
          <br>
          <strong>Mengdi Li</strong>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html">Cornelius Weber</a>,
          <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan Wermter</a>,
          <br>
          <em>ICANN</em>, 2020
          <br>
          <a href="https://www2.informatik.uni-hamburg.de/wtm/publications/2020/LWW20/Li_ICANN2020_.pdf">paper</a>
          <p></p>
          <p>
          We demonstrate that an efficient neural network designed for VQA can achieve high accuracy on detecting the relevance of questions to images, 
          however joint training the model on relevance detection and VQA leads to performance degradation on VQA.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src='images/sic-thumbnail.png' width="160">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Generating Steganographic Image Description by Dynamic Synonym Substitution</span>
          <br>
          <strong>Mengdi Li</strong>,
          Kai Mu,
          <a href="https://sci.cau.edu.cn/art/2018/3/23/art_30083_2.html">Ping Zhong</a>,
          <a href="http://faculty.cau.edu.cn/xxdqxy/wj_en/list.htm">Juan Wen</a>,
          <a href="http://faculty.cau.edu.cn/xxdqxy/xym/main.psp">Yiming Xue</a>,
          <br>
          <em>Signal Processing</em>, 2019
          <br>
          <a href="https://www.sciencedirect.com/science/article/abs/pii/S0165168419302233">paper</a>
          <p></p>
          <p>
          We propose a novel image captioning model to automatically generate stego image descriptions. 
          The proposed model is able to generate high-quality image descriptions in both human evaluation and statistical analysis.
          </p>
        </td>
      </tr>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  The template of this website is borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.               </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
